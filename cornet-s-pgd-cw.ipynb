{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2491748,"sourceType":"datasetVersion","datasetId":1500837},{"sourceId":88937,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":74619,"modelId":99381}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"1ef7df06-1111-40ba-9b12-d16a673318d2","_cell_guid":"438d8de6-95d4-4690-b49e-1676c34d8854","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset with labels","metadata":{"_uuid":"845b1bd8-3cc3-4c84-8a03-7583abb3761a","_cell_guid":"309305aa-0cc9-4721-b75c-7f896b8dcfc6","trusted":true}},{"cell_type":"code","source":"import json\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\n\n# Load labels from JSON file\nwith open('/kaggle/input/imagenet100/Labels.json', 'r') as f:\n    labels = json.load(f)\n\n# Create a mapping from class ID to class name\nid_to_class = {str(k): v for k, v in labels.items()}\n\n# Function to display an image with its label\ndef show_image_with_label(image_path, label):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(image)\n    plt.title(f'Label: {label}')\n    plt.axis('off')\n    plt.show()\n\n# Directories containing the images\nbase_train_dir = '/kaggle/input/imagenet100'\nval_dir = os.path.join(base_train_dir, 'val.X')\ntrain_dirs = [os.path.join(base_train_dir, f'train.X{i}') for i in range(1, 5)]\n\n# Helper function to get a random sample of images from a directory\ndef get_random_images(data_dirs, num_samples=5):\n    images = []\n    all_files = []\n    for data_dir in data_dirs:\n        for label_id in os.listdir(data_dir):\n            class_name = id_to_class.get(label_id, 'Unknown')\n            files = os.listdir(os.path.join(data_dir, label_id))\n            all_files.extend([(os.path.join(data_dir, label_id, filename), class_name) for filename in files])\n\n    selected_files = random.sample(all_files, num_samples)\n    for image_path, class_name in selected_files:\n        images.append((image_path, class_name))\n    return images\n\n# Get random images from validation set\nval_images = get_random_images([val_dir], 1)\n\n# Get random images from all training directories\ntrain_images = get_random_images(train_dirs, 1)\n\n# Display random validation images\nprint(\"Random Validation Images:\")\nfor image_path, label in val_images:\n    show_image_with_label(image_path, label)\n\n# Display random training images\nprint(\"Random Training Images:\")\nfor image_path, label in train_images:\n    show_image_with_label(image_path, label)","metadata":{"_uuid":"efcd7b75-c9f2-4acd-8136-be3f5c9f80cb","_cell_guid":"a0fbbff2-5de9-4346-9f3c-dd22f88a4c63","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting up Dataset and Dataloaders","metadata":{"_uuid":"305da4b2-1b9a-46f9-8d1d-e75198be8bf8","_cell_guid":"68402092-a8e0-464e-826f-a35f1020e95a","trusted":true}},{"cell_type":"code","source":"# Creating the transforms\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"_uuid":"023517e7-9307-4c95-a6c6-8919811e9330","_cell_guid":"9146b302-1b14-4c87-92a1-ea83c81a109d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a custom dataset class\nclass MultiFolderDataset(Dataset):\n    def __init__(self, folders, transform=None):\n        self.samples = []\n        self.transform = transform\n        for folder in folders:\n            for label_id in os.listdir(folder):\n                class_name = id_to_class.get(label_id, 'Unknown')\n                files = os.listdir(os.path.join(folder, label_id))\n                self.samples.extend([(os.path.join(folder, label_id, filename), label_id) for filename in files])\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(sorted(id_to_class.keys()))}\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        image = Image.open(path).convert(\"RGB\")\n        if self.transform is not None:\n            image = self.transform(image)\n        label = self.class_to_idx[label]\n        return image, label","metadata":{"_uuid":"acde311a-ca75-42c9-8774-221bf08d1ef6","_cell_guid":"aca76f28-5cee-481c-9d56-0aeec34be861","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.datasets as datasets\n# Creating the Datasets\ntrain_dataset = MultiFolderDataset(train_dirs, transform=transform)\nval_dataset = datasets.ImageFolder(val_dir, transform=transform)","metadata":{"_uuid":"00aa616a-cf74-4963-a8be-a3f64892d17e","_cell_guid":"ad29997d-c27f-4e3c-ae4b-4ae906c1976c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"_uuid":"5ccf26ca-d30f-4b43-9ade-e98b11f52837","_cell_guid":"b4a2934e-dfc6-4464-941e-34be88d2808e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting up the Model","metadata":{"_uuid":"843ad71b-cb4a-45c3-8971-e90dc350d6fc","_cell_guid":"db8fc88a-9ebf-4e08-a0d8-05072fa66b67","trusted":true}},{"cell_type":"code","source":"# Setting up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"_uuid":"4b42e5c1-444b-4b9b-bbb7-d2e1c1191e1a","_cell_guid":"3247f96a-074f-4e8c-a92f-b6aa7627fb8d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/dicarlolab/CORnet.git","metadata":{"_uuid":"d940e1e0-3de4-49dc-a47e-2374281850c1","_cell_guid":"b4a5303b-a9d9-4ba0-9a0f-6a86c24aed7f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Navigate to the cloned repository folder\nimport os\nos.chdir('/kaggle/working/CORnet')\n\n# Install the package if needed\n!pip install .","metadata":{"_uuid":"8525feb6-ec33-4ac4-8ed7-b6ff29fbaa64","_cell_guid":"329f28a6-04c4-4afd-aa62-7e576520444d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the CORnet-S model architecture\nfrom cornet import cornet_s\n\n# Initialize the CORnet-S model\nmodel = cornet_s()","metadata":{"_uuid":"41f29e1e-8f55-43ad-9dec-7ff7ed4ac977","_cell_guid":"418da783-b308-44a8-b98f-018beaf277e0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace the final layer to match ImageNet100 classes\nimport torch.nn as nn\n\nnum_classes = 100\nmodel.module.decoder.linear = nn.Linear(in_features=model.module.decoder.linear.in_features, out_features=num_classes)","metadata":{"_uuid":"fffca9b0-a6fd-4226-a88a-f6a96218dee7","_cell_guid":"07f9fda0-b2fe-4d47-a308-bbe2cfd3834b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"_uuid":"d3fa147b-2061-440d-9273-f0cfa690c874","_cell_guid":"c6455f53-d443-4eb8-9fa5-d1c0fb8baa2d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\n# Load the pre-trained weights\ncheckpoint = torch.load(\"/kaggle/input/cornet-s/pytorch/default/1/cornet_s-1d3f7974.pth\", map_location=device)\n\n# Remove the final layer from the checkpoint to avoid size mismatch\ndel checkpoint['state_dict']['module.decoder.linear.weight']\ndel checkpoint['state_dict']['module.decoder.linear.bias']\n\n# Load the rest of the weights into the model\nmodel.load_state_dict(checkpoint['state_dict'], strict=False)  # strict=False ignores the missing final layer","metadata":{"_uuid":"d904cf05-8d2a-40ce-9aae-31d63c308e2f","_cell_guid":"9185384e-5031-4915-a7df-ad1b4c2b36e9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\n\n# Function to evaluate model performance (pre-training and post-training)\ndef evaluate_model(model, loader):\n    model.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n    correct1 = 0\n    correct5 = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm.tqdm(loader, desc=\"Evaluating\"):  # Corrected tqdm usage\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)  # Forward pass\n            loss = criterion(outputs, labels)  # Compute loss\n\n            val_loss += loss.item() * inputs.size(0)  # Update validation loss\n\n            # Calculate top-1 and top-5 accuracy\n            _, pred1 = outputs.topk(1, 1, True, True)\n            _, pred5 = outputs.topk(5, 1, True, True)\n            correct1 += pred1.eq(labels.view(-1, 1).expand_as(pred1)).sum().item()\n            correct5 += pred5.eq(labels.view(-1, 1).expand_as(pred5)).sum().item()\n            total += labels.size(0)\n\n    epoch_val_loss = val_loss / len(loader.dataset)  # Compute validation loss\n    top1_acc = correct1 / total  # Compute top-1 accuracy\n    top5_acc = correct5 / total  # Compute top-5 accuracy\n\n    print(f'Validation Loss: {epoch_val_loss:.4f}')\n    print(f'Top-1 Accuracy: {top1_acc:.4f}, Top-5 Accuracy: {top5_acc:.4f}')\n    \n    return top1_acc, top5_acc\n\n# Initial evaluation of pre-trained model\nprint(\"Evaluating pre-trained model before fine-tuning:\")\nevaluate_model(model, val_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure CUDA is available\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# Setting up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Import necessary modules\nimport time\nfrom tqdm import tqdm\n\n# Move model to the appropriate device\nmodel = model.to(device)\n\n# Training loop\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n\n    # Training phase\n    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()  # Zero the parameter gradients\n\n        outputs = model(inputs)  # Forward pass\n        loss = criterion(outputs, labels)  # Compute loss\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n\n        running_loss += loss.item() * inputs.size(0)  # Update running loss\n\n        if i % 100 == 0:  # Print every 100 batches\n            print(f\"Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n    epoch_loss = running_loss / len(train_loader.dataset)  # Compute epoch loss\n\n    # Validation phase\n    model.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n    correct1 = 0\n    correct5 = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)  # Forward pass\n            loss = criterion(outputs, labels)  # Compute loss\n\n            val_loss += loss.item() * inputs.size(0)  # Update validation loss\n\n            # Calculate top-1 and top-5 accuracy\n            _, pred1 = outputs.topk(1, 1, True, True)\n            _, pred5 = outputs.topk(5, 1, True, True)\n            correct1 += pred1.eq(labels.view(-1, 1).expand_as(pred1)).sum().item()\n            correct5 += pred5.eq(labels.view(-1, 1).expand_as(pred5)).sum().item()\n            total += labels.size(0)\n\n    epoch_val_loss = val_loss / len(val_loader.dataset)  # Compute epoch validation loss\n    top1_acc = correct1 / total  # Compute top-1 accuracy\n    top5_acc = correct5 / total  # Compute top-5 accuracy\n\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n\n    print(f'Epoch [{epoch+1}/{num_epochs}] - Time: {epoch_duration:.2f}s')\n    print(f'Train Loss: {epoch_loss:.4f}')\n    print(f'Validation Loss: {epoch_val_loss:.4f}')\n    print(f'Top-1 Accuracy: {top1_acc:.4f}, Top-5 Accuracy: {top5_acc:.4f}')\n\n    # Save the model checkpoint\n    model_save_path = f'/kaggle/working/cornet_s_epoch{epoch+1}.pth'\n    torch.save(model.state_dict(), model_save_path)\n    print(f'Model saved at {model_save_path}')\n\nprint('Training complete')","metadata":{"_uuid":"6f40f5a8-9115-4b63-b4c8-1b7ad92aaaae","_cell_guid":"b3e98eeb-36bb-4fe8-983c-2b65317eba70","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing and Evaluating PGD attacks","metadata":{"_uuid":"544da8ea-ad13-44ee-b506-725c9d6766cd","_cell_guid":"7e96af24-a1e1-40b5-88f6-8cb58f806983","trusted":true}},{"cell_type":"code","source":"!pip install torchattacks","metadata":{"_uuid":"dba7f374-63cf-4803-bc9c-f44e2f56c0c1","_cell_guid":"e0a39661-2929-4799-972e-ef429caf7673","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchattacks\nfrom tqdm import tqdm\n\n# Assuming model, test_loader, and device are already defined\n\n# Initialize PGD attack\npgd_attack = torchattacks.PGD(model, eps=0.3, alpha=2/255, steps=40)\n\n# Evaluate model under PGD attack\npgd_correct = 0\npgd_total = 0\n\nmodel.eval()  # Set model to evaluation mode\nfor inputs, labels in tqdm(val_loader, desc=\"PGD Attack Evaluation\"):\n    inputs, labels = inputs.to(device), labels.to(device)\n\n    # Generate adversarial examples\n    adv_inputs = pgd_attack(inputs, labels)\n    \n    # Forward pass with adversarial examples\n    outputs = model(adv_inputs)\n    \n    # Calculate accuracy\n    _, predicted = outputs.max(1)\n    pgd_correct += predicted.eq(labels).sum().item()\n    pgd_total += labels.size(0)\n\n# PGD Attack accuracy\npgd_acc = pgd_correct / pgd_total\nprint(f'PGD Attack Accuracy: {pgd_acc:.4f}')","metadata":{"_uuid":"acfba399-9fb6-48a8-9fec-a2fc935963ca","_cell_guid":"e2acfb15-e7ba-4c52-8393-c3ccd40b8090","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing and Evaluating Carlini Wagner Attacks","metadata":{"_uuid":"f9529d54-6b70-434e-882d-cc50e555d154","_cell_guid":"43f9e0cf-675a-42e5-83ba-3eecad966ef8","trusted":true}},{"cell_type":"code","source":"# Initialize CW attack\ncw_attack = torchattacks.CW(model, c=1e-4, kappa=0, steps=500, lr=0.01)\n\n# Evaluate model under CW attack\ncw_correct = 0\ncw_total = 0\n\nmodel.eval()  # Set model to evaluation mode\nfor inputs, labels in tqdm(val_loader, desc=\"CW Attack Evaluation\"):\n    inputs, labels = inputs.to(device), labels.to(device)\n\n    # Generate adversarial examples\n    adv_inputs = cw_attack(inputs, labels)\n    \n    # Forward pass with adversarial examples\n    outputs = model(adv_inputs)\n    \n    # Calculate accuracy\n    _, predicted = outputs.max(1)\n    cw_correct += predicted.eq(labels).sum().item()\n    cw_total += labels.size(0)\n\n# CW Attack accuracy\ncw_acc = cw_correct / cw_total\nprint(f'CW Attack Accuracy: {cw_acc:.4f}')","metadata":{"_uuid":"b1c11d35-d632-4dae-b64c-582017a7731f","_cell_guid":"f2021ff3-0c81-42cc-a77d-ab7c300237d5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}